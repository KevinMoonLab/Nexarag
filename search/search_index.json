{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Nexarag Nexarag is an open-source platform for building knowledge graphs from research papers and querying them with AI, enabling transparent and reproducible literature analysis without the hallucinations of traditional RAG systems. Deploy locally with full privacy control or integrate with any LLM via the standardized Model Context Protocol (MCP). Usage Guides Quick Start Building Knowledge Graphs Exploring Knowledge Graphs Sharing Knowledge Graphs Advanced Configuration Development Guides Frontend Backend Design","title":"Welcome to Nexarag"},{"location":"#welcome-to-nexarag","text":"Nexarag is an open-source platform for building knowledge graphs from research papers and querying them with AI, enabling transparent and reproducible literature analysis without the hallucinations of traditional RAG systems. Deploy locally with full privacy control or integrate with any LLM via the standardized Model Context Protocol (MCP).","title":"Welcome to Nexarag"},{"location":"#usage-guides","text":"Quick Start Building Knowledge Graphs Exploring Knowledge Graphs Sharing Knowledge Graphs Advanced Configuration","title":"Usage Guides"},{"location":"#development-guides","text":"Frontend Backend Design","title":"Development Guides"},{"location":"development/architecture/","text":"System Overview Nexarag is a platform for exploring and interacting with research literature through dynamic knowledge graphs. It enables users to upload papers, generate semantic embeddings, and leverage these representations for intelligent querying and visual exploration. \u2728 Key Features Build Knowledge Graphs from references, citations, authors, venues, BibTex files, etc. Attach documents (e.g., full paper text) to graph nodes to create rich semantic embeddings and relationships Talk To Your Data (TTYD) via LLM-powered Retrieval-Augmented Generation (RAG) Semantic Visualizations like PCA projections of the semantic graph to explore research trajectories, gaps, and growth \ud83e\uddf1 Architecture Overview Nexarag consists of two primary layers: Frontend and Backend . All components are containerized using Docker and communicate asynchronously via RabbitMQ . \ud83d\udce6 Backend Architecture The backend is composed of three core services: 1. FastAPI Service Purpose : Exposes REST endpoints to handle frontend requests such as paper uploads, graph queries, and TTYD chat requests. Why FastAPI? : Chosen for its excellent performance, developer-friendly async support, and native support for OpenAPI documentation. Role in System : Acts as the interface layer, dispatching tasks to other backend services and coordinating responses to the frontend. 2. Database Service (Neo4j) Purpose : Stores the core knowledge graph, including relationships like cited by , authored by , and published in , as well as semantic embeddings for node content and associated documents. Uses neomodel as the Object-Graph Mapper (OGM) to simplify the research literature domain-specific language (DSL). Why Neo4j? : Graphs are the natural data model for this domain. Neo4j allows efficient traversal, querying, and expansion of literature networks. Semantic Embedding Support : Embeddings from the LLM are stored alongside graph nodes, enabling vector similarity search and semantic clustering. 3. Knowledge Graph Service Purpose : Handles all knowledge-centric operations \u2014 from querying Semantic Scholar to constructing knowledge graphs, computing embeddings, and processing TTYD (Talk To Your Data) interactions. Why a dedicated service? : Separating this logic allows for scalability and easier integration with different data sources and models. LLM Integration : Uses transformer-based language models to generate embeddings and power TTYD features via Retrieval-Augmented Generation (RAG). \ud83d\udd01 Inter-Service Communication Mechanism : All backend services communicate asynchronously using RabbitMQ . Why RabbitMQ? : Enables decoupled, event-driven processing of tasks like embedding generation, document parsing, and graph updates. Improves scalability and reliability by preventing tight coupling between services with high message durability. \ud83c\udf10 Frontend Architecture Built with Angular Visualizes knowledge graphs using Cytoscape.js Renders semantic plots using D3.js Connects to the backend via REST endpoints and web sockets provided by the FastAPI service \ud83e\udde0 Technology Stack Layer Technology Frontend Angular, Cytoscape.js, D3.js Backend API FastAPI (Python) Graph DB Neo4j, neomodel Messaging RabbitMQ Orchestration Docker + Docker Compose","title":"System Overview"},{"location":"development/architecture/#system-overview","text":"Nexarag is a platform for exploring and interacting with research literature through dynamic knowledge graphs. It enables users to upload papers, generate semantic embeddings, and leverage these representations for intelligent querying and visual exploration.","title":"System Overview"},{"location":"development/architecture/#key-features","text":"Build Knowledge Graphs from references, citations, authors, venues, BibTex files, etc. Attach documents (e.g., full paper text) to graph nodes to create rich semantic embeddings and relationships Talk To Your Data (TTYD) via LLM-powered Retrieval-Augmented Generation (RAG) Semantic Visualizations like PCA projections of the semantic graph to explore research trajectories, gaps, and growth","title":"\u2728 Key Features"},{"location":"development/architecture/#architecture-overview","text":"Nexarag consists of two primary layers: Frontend and Backend . All components are containerized using Docker and communicate asynchronously via RabbitMQ .","title":"\ud83e\uddf1 Architecture Overview"},{"location":"development/architecture/#backend-architecture","text":"The backend is composed of three core services:","title":"\ud83d\udce6 Backend Architecture"},{"location":"development/architecture/#1-fastapi-service","text":"Purpose : Exposes REST endpoints to handle frontend requests such as paper uploads, graph queries, and TTYD chat requests. Why FastAPI? : Chosen for its excellent performance, developer-friendly async support, and native support for OpenAPI documentation. Role in System : Acts as the interface layer, dispatching tasks to other backend services and coordinating responses to the frontend.","title":"1. FastAPI Service"},{"location":"development/architecture/#2-database-service-neo4j","text":"Purpose : Stores the core knowledge graph, including relationships like cited by , authored by , and published in , as well as semantic embeddings for node content and associated documents. Uses neomodel as the Object-Graph Mapper (OGM) to simplify the research literature domain-specific language (DSL). Why Neo4j? : Graphs are the natural data model for this domain. Neo4j allows efficient traversal, querying, and expansion of literature networks. Semantic Embedding Support : Embeddings from the LLM are stored alongside graph nodes, enabling vector similarity search and semantic clustering.","title":"2. Database Service (Neo4j)"},{"location":"development/architecture/#3-knowledge-graph-service","text":"Purpose : Handles all knowledge-centric operations \u2014 from querying Semantic Scholar to constructing knowledge graphs, computing embeddings, and processing TTYD (Talk To Your Data) interactions. Why a dedicated service? : Separating this logic allows for scalability and easier integration with different data sources and models. LLM Integration : Uses transformer-based language models to generate embeddings and power TTYD features via Retrieval-Augmented Generation (RAG).","title":"3. Knowledge Graph Service"},{"location":"development/architecture/#inter-service-communication","text":"Mechanism : All backend services communicate asynchronously using RabbitMQ . Why RabbitMQ? : Enables decoupled, event-driven processing of tasks like embedding generation, document parsing, and graph updates. Improves scalability and reliability by preventing tight coupling between services with high message durability.","title":"\ud83d\udd01 Inter-Service Communication"},{"location":"development/architecture/#frontend-architecture","text":"Built with Angular Visualizes knowledge graphs using Cytoscape.js Renders semantic plots using D3.js Connects to the backend via REST endpoints and web sockets provided by the FastAPI service","title":"\ud83c\udf10 Frontend Architecture"},{"location":"development/architecture/#technology-stack","text":"Layer Technology Frontend Angular, Cytoscape.js, D3.js Backend API FastAPI (Python) Graph DB Neo4j, neomodel Messaging RabbitMQ Orchestration Docker + Docker Compose","title":"\ud83e\udde0 Technology Stack"},{"location":"development/backend/","text":"Backend This is the backend stack consisting of three primary services ( api , kg , mcp ) and two libraries ( rabbit , scholar ) that communicate asynchronously over RabbitMQ: - api : FastAPI endpoints that service frontend queries and integrate with backend services. - mcp : FastMCP service that provides MCP access to the API and context generation for external LLM hosts. - kg : Primary service for creating embeddings, interacting with LLMs, and building visualizations. - rabbit : Defines the messaging layer between services. - scholar : Semantic Scholar integration. Local Development Development for the backend project can be done inside a devcontainer. The container will be automatically provisioned with a neo4j database, a rabbitmq instance, and the official Ollama container image. Copy the .env.example file into Nexarag/.devcontainer/.env , modifying the values for your system Open Nexarag in VS Code Press Ctrl+Shift+P and type Open Folder in Container Jupyter Lab will be run automatically inside nexarag.dev . Use the python 3.11.11 kernel to run Jupyter notebooks.","title":"Backend"},{"location":"development/backend/#backend","text":"This is the backend stack consisting of three primary services ( api , kg , mcp ) and two libraries ( rabbit , scholar ) that communicate asynchronously over RabbitMQ: - api : FastAPI endpoints that service frontend queries and integrate with backend services. - mcp : FastMCP service that provides MCP access to the API and context generation for external LLM hosts. - kg : Primary service for creating embeddings, interacting with LLMs, and building visualizations. - rabbit : Defines the messaging layer between services. - scholar : Semantic Scholar integration.","title":"Backend"},{"location":"development/backend/#local-development","text":"Development for the backend project can be done inside a devcontainer. The container will be automatically provisioned with a neo4j database, a rabbitmq instance, and the official Ollama container image. Copy the .env.example file into Nexarag/.devcontainer/.env , modifying the values for your system Open Nexarag in VS Code Press Ctrl+Shift+P and type Open Folder in Container Jupyter Lab will be run automatically inside nexarag.dev . Use the python 3.11.11 kernel to run Jupyter notebooks.","title":"Local Development"},{"location":"development/frontend/","text":"Frontend Frontend Angular application that provides Cytoscape.js and D3.js visualizations of Nexarag knowledge graphs. Also provides graph-building features, Semantic Scholar integration, and LLM talk-to-your-data (TTYD) chat capabilities. The frontend service in the compose stack serves the static Angular files and routes API requests via nginx . Local Development First Deployment Install NVM for your platform (Linux is recommended for local development) Install NPM In Nexarag/frontend , run npm install Running the Development Server Run the application stack from the root using docker compose . The API will be served at http://localhost:8000 In Nexarag/frontend , run npx nx s . The application will be available (with live reloads) at http://localhost:4200","title":"Frontend"},{"location":"development/frontend/#frontend","text":"Frontend Angular application that provides Cytoscape.js and D3.js visualizations of Nexarag knowledge graphs. Also provides graph-building features, Semantic Scholar integration, and LLM talk-to-your-data (TTYD) chat capabilities. The frontend service in the compose stack serves the static Angular files and routes API requests via nginx .","title":"Frontend"},{"location":"development/frontend/#local-development","text":"","title":"Local Development"},{"location":"development/frontend/#first-deployment","text":"Install NVM for your platform (Linux is recommended for local development) Install NPM In Nexarag/frontend , run npm install","title":"First Deployment"},{"location":"development/frontend/#running-the-development-server","text":"Run the application stack from the root using docker compose . The API will be served at http://localhost:8000 In Nexarag/frontend , run npx nx s . The application will be available (with live reloads) at http://localhost:4200","title":"Running the Development Server"},{"location":"usage/build/","text":"Building Knowledge Graphs Semantic Scholar Nexarag uses Semantic Scholar to populate author and publication data, and to search for relevant papers. 1. Click the plus menu item in the left sidebar. 2. Enter a search term. 3. Select papers from the search results to add to the graph. 4. Click 'Add Data' to add nodes to the graph. BibTex Nexarag can ingest BibTex files and produce a graph enriched with data from Semantic Scholar. 1. Click the plus menu item in the left sidebar. 2. Select the BibTex tab. 3. Paste in the contents of your BibTex file. 4. Click submit. 5. After all papers have been referenced in Semantic Scholar, they will appear automatically in the graph. Document Upload Documents can be added to the graph as singleton nodes or associated with existing papers. Add Document to Paper Node Right click on a blue Paper node in the graph. Click 'Add Documents' Click 'Choose' and select a document. Click 'Upload' to upload the file. The node will be automatically added to your graph, and the file is accessible by right clicking on the purple document node and selecting 'Details'. Embeddings are generated automatically. References & Citations Knowledge graphs can be further enriched by adding citations and references for specific papers. Right click on a blue paper node. Select 'Add Citations' or 'Add References' Wait for the graph to populate with data from Semantic Scholar.","title":"Building Knowledge Graphs"},{"location":"usage/build/#building-knowledge-graphs","text":"","title":"Building Knowledge Graphs"},{"location":"usage/build/#semantic-scholar","text":"Nexarag uses Semantic Scholar to populate author and publication data, and to search for relevant papers. 1. Click the plus menu item in the left sidebar. 2. Enter a search term. 3. Select papers from the search results to add to the graph. 4. Click 'Add Data' to add nodes to the graph.","title":"Semantic Scholar"},{"location":"usage/build/#bibtex","text":"Nexarag can ingest BibTex files and produce a graph enriched with data from Semantic Scholar. 1. Click the plus menu item in the left sidebar. 2. Select the BibTex tab. 3. Paste in the contents of your BibTex file. 4. Click submit. 5. After all papers have been referenced in Semantic Scholar, they will appear automatically in the graph.","title":"BibTex"},{"location":"usage/build/#document-upload","text":"Documents can be added to the graph as singleton nodes or associated with existing papers.","title":"Document Upload"},{"location":"usage/build/#add-document-to-paper-node","text":"Right click on a blue Paper node in the graph. Click 'Add Documents' Click 'Choose' and select a document. Click 'Upload' to upload the file. The node will be automatically added to your graph, and the file is accessible by right clicking on the purple document node and selecting 'Details'. Embeddings are generated automatically.","title":"Add Document to Paper Node"},{"location":"usage/build/#references-citations","text":"Knowledge graphs can be further enriched by adding citations and references for specific papers. Right click on a blue paper node. Select 'Add Citations' or 'Add References' Wait for the graph to populate with data from Semantic Scholar.","title":"References &amp; Citations"},{"location":"usage/config/","text":"Advanced Configuration A number of advanced settings can be applied using environment variables. Copy .env.example into the same folder as your docker compose file, and modify the values as needed. Environment Variables Name Default Description COMPOSE_PROJECT_NAME nexarag Overrides the default Docker Compose project name to group containers under a unified prefix. NEO4J_USERNAME neo4j Username for authenticating with the Neo4j database. NEO4J_PASSWORD password Password for the Neo4j user. RABBITMQ_USERNAME guest Username for connecting to RabbitMQ. RABBITMQ_PASSWORD guest Password for the RabbitMQ user. API_PORT 8000 Port exposed by the API service. FRONTEND_PORT 5000 Port for serving the frontend application. OLLAMA_PORT 11434 Port used by the Ollama server for model inference. MCP_PORT 9000 Port for the MCP service. EMBEDDING_CHUNK_SIZE 500 Size of text chunks (in characters or tokens, depending on implementation) used before generating embeddings. EMBEDDING_CHUNK_OVERLAP 100 Number of overlapping characters/tokens between embedding chunks to preserve context. EMBEDDING_MODEL nomic-embed-text:v1.5 Model identifier used for generating vector embeddings.","title":"Advanced Configuration"},{"location":"usage/config/#advanced-configuration","text":"A number of advanced settings can be applied using environment variables. Copy .env.example into the same folder as your docker compose file, and modify the values as needed.","title":"Advanced Configuration"},{"location":"usage/config/#environment-variables","text":"Name Default Description COMPOSE_PROJECT_NAME nexarag Overrides the default Docker Compose project name to group containers under a unified prefix. NEO4J_USERNAME neo4j Username for authenticating with the Neo4j database. NEO4J_PASSWORD password Password for the Neo4j user. RABBITMQ_USERNAME guest Username for connecting to RabbitMQ. RABBITMQ_PASSWORD guest Password for the RabbitMQ user. API_PORT 8000 Port exposed by the API service. FRONTEND_PORT 5000 Port for serving the frontend application. OLLAMA_PORT 11434 Port used by the Ollama server for model inference. MCP_PORT 9000 Port for the MCP service. EMBEDDING_CHUNK_SIZE 500 Size of text chunks (in characters or tokens, depending on implementation) used before generating embeddings. EMBEDDING_CHUNK_OVERLAP 100 Number of overlapping characters/tokens between embedding chunks to preserve context. EMBEDDING_MODEL nomic-embed-text:v1.5 Model identifier used for generating vector embeddings.","title":"Environment Variables"},{"location":"usage/explore/","text":"Exploring Knowledge Graphs There are three ways to explore Nexarag knowledge graphs: * Built-in LLM chat feature * Interactive visualization tools * Filtering LLM Chat Feature The Nexarag chat feature, available in the right sidebar, uses context from paper abstracts, documents, authors, and other data and relationships in the knowledge graph to respond to user queries. Click the chat menu item in the right sidebar. [Optional] Click the cog on the right sidebar and choose a different Ollama model or specify a different system prompt. Ask a question about the knowledge graph! Interactive Visualizations Nexarag allows users to generate two-dimensional visualizations of the abstracts and documents in their knowledge graph. Click the chart icon on the left sidebar. Select a color variable. (Currently only 'Labels' is supported). Add a label for your prompt. Add a prompt, e.g. \"Papers about constrained optimization\". Repeat steps 3 and 4 for as many labels as is desired. Click 'Submit'. Your plot will be generated automatically. Filtering Users can filter the graph and change certain display settings. Click the sliders icon on the left sidebar. Filter Description Search Enter a search term to filter by title. Visible Nodes Filter visible nodes: Paper, Author, Document, Journal, and Publication Venue. Node Weighting Nodes will be weighted by the number of edges they have. Node Repulsion Used by the Cytoscape COSE layout.","title":"Exploring Knowledge Graphs"},{"location":"usage/explore/#exploring-knowledge-graphs","text":"There are three ways to explore Nexarag knowledge graphs: * Built-in LLM chat feature * Interactive visualization tools * Filtering","title":"Exploring Knowledge Graphs"},{"location":"usage/explore/#llm-chat-feature","text":"The Nexarag chat feature, available in the right sidebar, uses context from paper abstracts, documents, authors, and other data and relationships in the knowledge graph to respond to user queries. Click the chat menu item in the right sidebar. [Optional] Click the cog on the right sidebar and choose a different Ollama model or specify a different system prompt. Ask a question about the knowledge graph!","title":"LLM Chat Feature"},{"location":"usage/explore/#interactive-visualizations","text":"Nexarag allows users to generate two-dimensional visualizations of the abstracts and documents in their knowledge graph. Click the chart icon on the left sidebar. Select a color variable. (Currently only 'Labels' is supported). Add a label for your prompt. Add a prompt, e.g. \"Papers about constrained optimization\". Repeat steps 3 and 4 for as many labels as is desired. Click 'Submit'. Your plot will be generated automatically.","title":"Interactive Visualizations"},{"location":"usage/explore/#filtering","text":"Users can filter the graph and change certain display settings. Click the sliders icon on the left sidebar. Filter Description Search Enter a search term to filter by title. Visible Nodes Filter visible nodes: Paper, Author, Document, Journal, and Publication Venue. Node Weighting Nodes will be weighted by the number of edges they have. Node Repulsion Used by the Cytoscape COSE layout.","title":"Filtering"},{"location":"usage/share/","text":"Sharing Knowledge Graphs The Nexarag graph manager feature allows users to export, save, and switch between different Neo4j knowledge graphs in the Nexarag platform. Overview The Knowledge Graph Management feature provides: Export Current KG : Save the current knowledge graph to a dump file. Import/Load KG : Switch to a previously saved knowledge graph. List Available KGs : View all saved knowledge graphs with metadata. Delete KGs : Remove unwanted knowledge graph dumps. Quick Selector : Dropdown menu to quickly switch between knowledge graphs. Quick Start Exporting Knowledge Graphs Click the cog menu item to access the projects workspace. Enter a name for your export. [Optional] Add a description. Click \"Export Knowledge Graph\" The current Neo4j database will be saved as a dump file to kg_dumps in the same folder as the Docker compose file. Import Knowledge Graphs Copy the kg_dumps file to the target, in the same folder as the Docker compose file. Run docker compose restart . In the Nexarag frontend, open the projects workspace. Identify the desired export and click \"Load\". Switching Knowledge Graphs Option 1: Quick Selector 1. Use the dropdown at the top of the screen. 2. Select a knowledge graph from the list. 3. The system will automatically load the selected KG. Option 2: Management Interface 1. Open the projects workspace. 2. Click \"Load\" button on any knowledge graph card. 3. Confirm the action. Deleting Knowledge Graphs Open the projects workspace. Click \"Delete\" button on the knowledge graph card Confirm the deletion in the popup dialog. Backend Implementation API Endpoints GET /kg/list/ - List all available knowledge graph dumps POST /kg/export/ - Export current knowledge graph POST /kg/import/ - Import/load a knowledge graph DELETE /kg/delete/ - Delete a knowledge graph dump GET /kg/current/ - Get current knowledge graph information Docker Changes Added kg_dumps volume to all docker-compose files: - docker-compose.cpu.yml - docker-compose.gpu.yml - docker-compose.macos.yml This volume is mounted at /dumps in the Neo4j container and shared with API and KG services. KnowledgeGraphManager Class Located in backend/src/kg/db/kg_manager.py , this class handles: - Exporting databases using neo4j-admin database dump - Loading databases using neo4j-admin database load - Managing metadata for saved knowledge graphs - File operations for dump files Frontend Implementation New Components KgManagementComponent ( frontend/src/app/kg/kg-management.component.ts ) Full knowledge graph management interface Export form with name and description Grid view of available knowledge graphs Load/delete actions for each KG KgSelectorComponent ( frontend/src/app/kg/kg-selector.component.ts ) Compact dropdown selector Quick switching between knowledge graphs Positioned at top of main viewport KnowledgeGraphService ( frontend/src/app/kg/kg.service.ts ) HTTP service for KG API calls TypeScript interfaces for API responses UI Integration Added new \"Knowledge Graphs\" tab to the main menu (database icon) Added KG selector dropdown at the top of the main viewport Toast notifications for all operations Confirmation dialogs for destructive actions Technical Notes Neo4j Commands The system uses Neo4j 5.x admin commands: - Export: neo4j-admin database dump --to-path=/dumps --database=<name> --overwrite-destination=true - Import: neo4j-admin database load --from-path=/dumps --database=<name> --overwrite-destination=true Database Operations When loading a knowledge graph: 1. Neo4j database is stopped 2. Dump file is loaded 3. Database is restarted This ensures data consistency but causes a brief service interruption. Metadata Storage Knowledge graph metadata is stored in kg_metadata.json within the dumps volume: { \"kg_name\": { \"created_at\": \"ISO datetime\", \"description\": \"User description\", \"database\": \"neo4j database name\" } } Volume Persistence The kg_dumps volume persists knowledge graphs across container restarts. However, if you remove the volume, all saved knowledge graphs will be lost. Limitations Single Database : Currently supports one active database at a time Service Interruption : Loading a KG briefly stops the Neo4j service Storage Space : Large knowledge graphs require significant storage No Compression : Dump files are stored uncompressed Future Enhancements Parallel Databases : Support multiple concurrent databases Compression : Compress dump files to save space Import/Export Progress : Show progress bars for large operations KG Comparison : Compare differences between knowledge graphs Scheduled Exports : Automatic periodic backups Cloud Storage : Upload/download KGs to/from cloud storage","title":"Sharing Knowledge Graphs"},{"location":"usage/share/#sharing-knowledge-graphs","text":"The Nexarag graph manager feature allows users to export, save, and switch between different Neo4j knowledge graphs in the Nexarag platform.","title":"Sharing Knowledge Graphs"},{"location":"usage/share/#overview","text":"The Knowledge Graph Management feature provides: Export Current KG : Save the current knowledge graph to a dump file. Import/Load KG : Switch to a previously saved knowledge graph. List Available KGs : View all saved knowledge graphs with metadata. Delete KGs : Remove unwanted knowledge graph dumps. Quick Selector : Dropdown menu to quickly switch between knowledge graphs.","title":"Overview"},{"location":"usage/share/#quick-start","text":"","title":"Quick Start"},{"location":"usage/share/#exporting-knowledge-graphs","text":"Click the cog menu item to access the projects workspace. Enter a name for your export. [Optional] Add a description. Click \"Export Knowledge Graph\" The current Neo4j database will be saved as a dump file to kg_dumps in the same folder as the Docker compose file.","title":"Exporting Knowledge Graphs"},{"location":"usage/share/#import-knowledge-graphs","text":"Copy the kg_dumps file to the target, in the same folder as the Docker compose file. Run docker compose restart . In the Nexarag frontend, open the projects workspace. Identify the desired export and click \"Load\".","title":"Import Knowledge Graphs"},{"location":"usage/share/#switching-knowledge-graphs","text":"Option 1: Quick Selector 1. Use the dropdown at the top of the screen. 2. Select a knowledge graph from the list. 3. The system will automatically load the selected KG. Option 2: Management Interface 1. Open the projects workspace. 2. Click \"Load\" button on any knowledge graph card. 3. Confirm the action.","title":"Switching Knowledge Graphs"},{"location":"usage/share/#deleting-knowledge-graphs","text":"Open the projects workspace. Click \"Delete\" button on the knowledge graph card Confirm the deletion in the popup dialog.","title":"Deleting Knowledge Graphs"},{"location":"usage/share/#backend-implementation","text":"","title":"Backend Implementation"},{"location":"usage/share/#api-endpoints","text":"GET /kg/list/ - List all available knowledge graph dumps POST /kg/export/ - Export current knowledge graph POST /kg/import/ - Import/load a knowledge graph DELETE /kg/delete/ - Delete a knowledge graph dump GET /kg/current/ - Get current knowledge graph information","title":"API Endpoints"},{"location":"usage/share/#docker-changes","text":"Added kg_dumps volume to all docker-compose files: - docker-compose.cpu.yml - docker-compose.gpu.yml - docker-compose.macos.yml This volume is mounted at /dumps in the Neo4j container and shared with API and KG services.","title":"Docker Changes"},{"location":"usage/share/#knowledgegraphmanager-class","text":"Located in backend/src/kg/db/kg_manager.py , this class handles: - Exporting databases using neo4j-admin database dump - Loading databases using neo4j-admin database load - Managing metadata for saved knowledge graphs - File operations for dump files","title":"KnowledgeGraphManager Class"},{"location":"usage/share/#frontend-implementation","text":"","title":"Frontend Implementation"},{"location":"usage/share/#new-components","text":"KgManagementComponent ( frontend/src/app/kg/kg-management.component.ts ) Full knowledge graph management interface Export form with name and description Grid view of available knowledge graphs Load/delete actions for each KG KgSelectorComponent ( frontend/src/app/kg/kg-selector.component.ts ) Compact dropdown selector Quick switching between knowledge graphs Positioned at top of main viewport KnowledgeGraphService ( frontend/src/app/kg/kg.service.ts ) HTTP service for KG API calls TypeScript interfaces for API responses","title":"New Components"},{"location":"usage/share/#ui-integration","text":"Added new \"Knowledge Graphs\" tab to the main menu (database icon) Added KG selector dropdown at the top of the main viewport Toast notifications for all operations Confirmation dialogs for destructive actions","title":"UI Integration"},{"location":"usage/share/#technical-notes","text":"","title":"Technical Notes"},{"location":"usage/share/#neo4j-commands","text":"The system uses Neo4j 5.x admin commands: - Export: neo4j-admin database dump --to-path=/dumps --database=<name> --overwrite-destination=true - Import: neo4j-admin database load --from-path=/dumps --database=<name> --overwrite-destination=true","title":"Neo4j Commands"},{"location":"usage/share/#database-operations","text":"When loading a knowledge graph: 1. Neo4j database is stopped 2. Dump file is loaded 3. Database is restarted This ensures data consistency but causes a brief service interruption.","title":"Database Operations"},{"location":"usage/share/#metadata-storage","text":"Knowledge graph metadata is stored in kg_metadata.json within the dumps volume: { \"kg_name\": { \"created_at\": \"ISO datetime\", \"description\": \"User description\", \"database\": \"neo4j database name\" } }","title":"Metadata Storage"},{"location":"usage/share/#volume-persistence","text":"The kg_dumps volume persists knowledge graphs across container restarts. However, if you remove the volume, all saved knowledge graphs will be lost.","title":"Volume Persistence"},{"location":"usage/share/#limitations","text":"Single Database : Currently supports one active database at a time Service Interruption : Loading a KG briefly stops the Neo4j service Storage Space : Large knowledge graphs require significant storage No Compression : Dump files are stored uncompressed","title":"Limitations"},{"location":"usage/share/#future-enhancements","text":"Parallel Databases : Support multiple concurrent databases Compression : Compress dump files to save space Import/Export Progress : Show progress bars for large operations KG Comparison : Compare differences between knowledge graphs Scheduled Exports : Automatic periodic backups Cloud Storage : Upload/download KGs to/from cloud storage","title":"Future Enhancements"}]}